{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Read In and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>previous_word</th>\n",
       "      <th>one_word</th>\n",
       "      <th>one_word_next</th>\n",
       "      <th>two_word</th>\n",
       "      <th>two_word_next</th>\n",
       "      <th>three_word</th>\n",
       "      <th>three_word_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>\\n</td>\n",
       "      <td>from</td>\n",
       "      <td>fairest</td>\n",
       "      <td>('from', 'fairest')</td>\n",
       "      <td>creatures</td>\n",
       "      <td>('from', 'fairest', 'creatures')</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>from</td>\n",
       "      <td>fairest</td>\n",
       "      <td>creatures</td>\n",
       "      <td>('fairest', 'creatures')</td>\n",
       "      <td>we</td>\n",
       "      <td>('fairest', 'creatures', 'we')</td>\n",
       "      <td>desire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>fairest</td>\n",
       "      <td>creatures</td>\n",
       "      <td>we</td>\n",
       "      <td>('creatures', 'we')</td>\n",
       "      <td>desire</td>\n",
       "      <td>('creatures', 'we', 'desire')</td>\n",
       "      <td>increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>creatures</td>\n",
       "      <td>we</td>\n",
       "      <td>desire</td>\n",
       "      <td>('we', 'desire')</td>\n",
       "      <td>increase</td>\n",
       "      <td>('we', 'desire', 'increase')</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>we</td>\n",
       "      <td>desire</td>\n",
       "      <td>increase</td>\n",
       "      <td>('desire', 'increase')</td>\n",
       "      <td>,</td>\n",
       "      <td>('desire', 'increase', ',')</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>desire</td>\n",
       "      <td>increase</td>\n",
       "      <td>,</td>\n",
       "      <td>('increase', ',')</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>increase</td>\n",
       "      <td>,</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>\\n</td>\n",
       "      <td>that</td>\n",
       "      <td>thereby</td>\n",
       "      <td>('that', 'thereby')</td>\n",
       "      <td>beauty's</td>\n",
       "      <td>('that', 'thereby', \"beauty's\")</td>\n",
       "      <td>rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>that</td>\n",
       "      <td>thereby</td>\n",
       "      <td>beauty's</td>\n",
       "      <td>('thereby', \"beauty's\")</td>\n",
       "      <td>rose</td>\n",
       "      <td>('thereby', \"beauty's\", 'rose')</td>\n",
       "      <td>might</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>thereby</td>\n",
       "      <td>beauty's</td>\n",
       "      <td>rose</td>\n",
       "      <td>(\"beauty's\", 'rose')</td>\n",
       "      <td>might</td>\n",
       "      <td>(\"beauty's\", 'rose', 'might')</td>\n",
       "      <td>never</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  previous_word   one_word one_word_next                  two_word  \\\n",
       "0            \\n       from       fairest       ('from', 'fairest')   \n",
       "1          from    fairest     creatures  ('fairest', 'creatures')   \n",
       "2       fairest  creatures            we       ('creatures', 'we')   \n",
       "3     creatures         we        desire          ('we', 'desire')   \n",
       "4            we     desire      increase    ('desire', 'increase')   \n",
       "5        desire   increase             ,         ('increase', ',')   \n",
       "6      increase          ,            \\n                        \\n   \n",
       "7            \\n       that       thereby       ('that', 'thereby')   \n",
       "8          that    thereby      beauty's   ('thereby', \"beauty's\")   \n",
       "9       thereby   beauty's          rose      (\"beauty's\", 'rose')   \n",
       "\n",
       "  two_word_next                        three_word three_word_next  \n",
       "0     creatures  ('from', 'fairest', 'creatures')              we  \n",
       "1            we    ('fairest', 'creatures', 'we')          desire  \n",
       "2        desire     ('creatures', 'we', 'desire')        increase  \n",
       "3      increase      ('we', 'desire', 'increase')               ,  \n",
       "4             ,       ('desire', 'increase', ',')              \\n  \n",
       "5            \\n                                \\n              \\n  \n",
       "6            \\n                                \\n              \\n  \n",
       "7      beauty's   ('that', 'thereby', \"beauty's\")            rose  \n",
       "8          rose   ('thereby', \"beauty's\", 'rose')           might  \n",
       "9         might     (\"beauty's\", 'rose', 'might')           never  "
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = pd.read_csv(\"../data/word_sequences.csv\")\n",
    "\n",
    "sequences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropping = sequences[sequences[\"three_word\"] == \"\\n\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences.drop(index = dropping, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting joys never end \n",
    "# each list was saved as one string, so each thing has to be broken up into individual segments again \n",
    "# each segment has to be scrubbed of extraneous puncuation \n",
    "\n",
    "three_word = []\n",
    "\n",
    "for line in sequences[\"three_word\"]: \n",
    "    line_list = line.split(\" \")\n",
    "    line_formatted = []\n",
    "    for word in line_list: \n",
    "        w = word.replace(\")\", \"\").replace(\"(\", \"\")\n",
    "        if len(w) == 3 and w.count(',') == 1: \n",
    "            line_formatted.append(w.replace(\"'\", \"\"))\n",
    "        elif w.count(\"'\") == 1: \n",
    "            line_formatted.append(w.replace(\",\", \"\").replace('\"', \"\"))\n",
    "        else: \n",
    "            line_formatted.append(w.replace(\"'\", \"\").replace(\",\", \"\"))\n",
    "    three_word.append(line_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(three_word)\n",
    "\n",
    "tokenizer.texts_to_matrix(three_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3217"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3217"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3217"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_vec = []\n",
    "\n",
    "for seq in three_word: \n",
    "    seq_vec = []\n",
    "    for word in seq:\n",
    "        \n",
    "        vector = [0] * (len(tokenizer.word_index) + 1)\n",
    "        vector[tokenizer.word_index[word]] = 1\n",
    "        \n",
    "        seq_vec.append(vector)\n",
    "        \n",
    "    mega_vec.append(seq_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_word_vecs_FIXED = np.array(mega_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use keras's Tokenizer to one-hot encode three word sequences and next word list\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(three_word)\n",
    "\n",
    "three_word_vecs = []\n",
    "for seq in three_word: \n",
    "    three_word_vecs.append(tokenizer.texts_to_matrix(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_vecs = tokenizer.texts_to_matrix(sequences[\"three_word_next\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16003"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next_word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16003, 3, 3218)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_word_vecs_FIXED.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "259\n",
      "desire\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "\n",
    "print(next_word_vecs[a].sum())\n",
    "print(next_word_vecs[a].argmax())\n",
    "print(tokenizer.index_word[next_word_vecs[a].argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "\n",
    "train_vecs = three_word_vecs_FIXED[:13000]\n",
    "train_next_word = next_word_vecs[:13000]\n",
    "\n",
    "validation_vecs = three_word_vecs_FIXED[13000:]\n",
    "validation_next_word = next_word_vecs[13000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3219)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(512, input_shape = (3, 3218), return_sequences = True))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dense(3218, activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", \n",
    "              optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13000 samples, validate on 3003 samples\n",
      "Epoch 1/5\n",
      " - 13s - loss: 5.5759 - val_loss: 4.9377\n",
      "Epoch 2/5\n",
      " - 12s - loss: 4.7940 - val_loss: 4.9406\n",
      "Epoch 3/5\n",
      " - 13s - loss: 4.6888 - val_loss: 5.2893\n",
      "Epoch 4/5\n",
      " - 13s - loss: 5.0610 - val_loss: 8.3644\n",
      "Epoch 5/5\n",
      " - 13s - loss: 5.8783 - val_loss: 10.3069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x3cc89ab90>"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = train_vecs, y = train_next_word, \n",
    "          verbose = 2, \n",
    "          epochs = 5, \n",
    "          batch_size = 512, \n",
    "          validation_data = (validation_vecs, validation_next_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(validation_vecs[96].reshape(1, 3, 3218))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wardrobe\n",
      "0.26633182\n",
      "\n",
      "with\n",
      "0.25500572\n",
      "\n",
      "takes\n",
      "0.14803708\n",
      "\n",
      "see\n",
      "0.060847066\n",
      "\n",
      "thine\n",
      "0.059176918\n",
      "\n",
      "as\n",
      "0.039178062\n",
      "\n",
      "still\n",
      "0.030482126\n",
      "\n",
      "live\n",
      "0.021617701\n",
      "\n",
      "the\n",
      "0.014018053\n",
      "\n",
      "say\n",
      "0.013250172\n",
      "\n",
      "replete\n",
      "0.011316883\n",
      "\n",
      "woe\n",
      "0.010734247\n",
      "\n",
      "sweet\n",
      "0.009608619\n",
      "\n",
      "brow\n",
      "0.009565318\n",
      "\n",
      "added\n",
      "0.009532099\n",
      "\n",
      "at\n",
      "0.008029771\n",
      "\n",
      "could\n",
      "0.0043166927\n",
      "\n",
      "lend\n",
      "0.0042862385\n",
      "\n",
      "false\n",
      "0.0037006107\n",
      "\n",
      "thee\n",
      "0.0021617115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in pred[0].argsort()[-20:][::-1]: \n",
    "    print(tokenizer.index_word[i])\n",
    "    print(pred[0][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'takes'"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word[83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(validation_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3003, 3218)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tender'"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word[validation_vecs[123].argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUCK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(wurd_list): \n",
    "    feed_vec = [] \n",
    "    for wurd in wurd_list: \n",
    "        \n",
    "        index = tokenizer.word_index[wurd]\n",
    "    \n",
    "        # create array of all zeroes, insert 1 at index of the word \n",
    "    \n",
    "        zeroes = [0] * (len(tokenizer.index_word) + 1)\n",
    "        zeroes[index] = 1\n",
    "        \n",
    "        feed_vec.append(zeroes)\n",
    "        \n",
    "    feed_arr = np.array(feed_vec)\n",
    "    \n",
    "    # feed that array into the model with reshaping \n",
    "    prediction = model.predict(feed_arr.reshape(1, 3, 3218))\n",
    "    \n",
    "    for i in prediction[0].argsort()[-20:][::-1]: \n",
    "        print(tokenizer.index_word[i])\n",
    "        print(pred[0][i])\n",
    "        print()\n",
    "\n",
    "#     # take prediction and decipher it \n",
    "#     next = tokenizer.index_word[prediction.argmax()]\n",
    "    \n",
    "#     return f\"{wurd_list[0]} {wurd_list[1]} {wurd_list[2]} {next}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "0.0\n",
      "\n",
      "are\n",
      "6.725015e-17\n",
      "\n",
      "with\n",
      "0.25500572\n",
      "\n",
      "of\n",
      "2.728745e-31\n",
      "\n",
      "joy\n",
      "1.5509508e-12\n",
      "\n",
      "them\n",
      "0.0\n",
      "\n",
      "in\n",
      "1.1132816e-08\n",
      "\n",
      "an\n",
      "8.037333e-20\n",
      "\n",
      "to\n",
      "8.4594585e-23\n",
      "\n",
      "night\n",
      "2.1101032e-05\n",
      "\n",
      "not\n",
      "1.6544083e-12\n",
      "\n",
      "spring\n",
      "0.0\n",
      "\n",
      "where\n",
      "7.958371e-06\n",
      "\n",
      "mind\n",
      "0.00035446548\n",
      "\n",
      "show\n",
      "9.999511e-09\n",
      "\n",
      "that\n",
      "5.229358e-11\n",
      "\n",
      "away\n",
      "2.1234494e-29\n",
      "\n",
      "given\n",
      "0.0\n",
      "\n",
      "birth\n",
      "0.0\n",
      "\n",
      "one\n",
      "5.5795483e-11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "next_word([\"thine\", \"creatures\", \"wardrobe\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying: Predictions based on one previous word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word = sequences[\"one_word\"]\n",
    "one_next = sequences[\"one_word_next\"]\n",
    "\n",
    "tokenizer_one = Tokenizer()\n",
    "tokenizer_one.fit_on_texts(one_word)\n",
    "\n",
    "one_word_vecs = tokenizer_one.texts_to_matrix(one_word)\n",
    "one_next_vecs = tokenizer_one.texts_to_matrix(one_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20315, 3210)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_word_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3210"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(one_word_vecs[0] == one_word_vecs[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3209"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(one_next_vecs[230] == one_next_vecs[891]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_one.word_counts[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_vecs = one_word_vecs[:16000]\n",
    "train_next_vecs = one_next_vecs[:16000]\n",
    "\n",
    "validation_one_vecs = one_word_vecs[16000:]\n",
    "validation_next_vecs = one_next_vecs[16000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(256, input_shape = (1, 3210)))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dense(3210, activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", \n",
    "              optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = [EarlyStopping(patience = 3, monitor = \"val_loss\", verbose = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairest'"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_one.index_word[train_one_vecs[1].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'creatures'"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_one.index_word[train_next_vecs[1].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4315 samples\n",
      "Epoch 1/30\n",
      " - 5s - loss: 5.8083 - val_loss: 5.1603\n",
      "Epoch 2/30\n",
      " - 5s - loss: 5.0001 - val_loss: 5.1831\n",
      "Epoch 3/30\n",
      " - 5s - loss: 4.9340 - val_loss: 5.4405\n",
      "Epoch 4/30\n",
      " - 5s - loss: 5.0678 - val_loss: 7.1492\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x36ba3f590>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = train_one_vecs.reshape(16000, 1, 3210), y = train_next_vecs, \n",
    "          verbose = 2, \n",
    "          epochs = 30, \n",
    "          batch_size = 256, \n",
    "          validation_data = (validation_one_vecs.reshape(4315, 1, 3210), validation_next_vecs), \n",
    "          callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(validation_one_vecs.reshape(4315, 1, 3210))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01621103"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  8, 19, 13,  2])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[100].argsort()[-5:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[97].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_one_vecs[4].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me'"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_one.index_word[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
