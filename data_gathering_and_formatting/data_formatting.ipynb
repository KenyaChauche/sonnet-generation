{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formatting\n",
    "\n",
    "Data is formatted and annotated for use with various models later in the project. Reformatting includes annotating lines with syllable counts of words, breaking lines up into work sequences, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import cmudict as cmu\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "phones = cmu.dict()\n",
    "\n",
    "def syllable_count(word): \n",
    "    try: \n",
    "        if word == \",\" or word == \".\" or word == \"!\" or word == \";\" or word == \"-\" or word == \"/\" or word == \"\\n\": \n",
    "            return 0\n",
    "        elif word == \" \": \n",
    "            pass\n",
    "        else: \n",
    "            return sum([char.isdigit() for block in phones[f\"{word}\"][0] for char in block])    \n",
    "    except: \n",
    "        return None\n",
    "    \n",
    "# thanks to this Stack Exchange post for the inspiration \n",
    "# https://datascience.stackexchange.com/questions/23376/how-to-get-the-number-of-syllables-in-a-word\n",
    "\n",
    "\n",
    "def tuplize(word): \n",
    "    return (word, syllable_count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('self-substantiatlize', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuplize(\"self-substantiatlize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "couplets = pd.read_csv(\"../data/couplets.csv\")\n",
    "\n",
    "lines = pd.read_csv(\"../data/lines.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting Couplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemizing each couplet, keeping the puncuation \n",
    "\n",
    "itemized = []\n",
    "\n",
    "for i in range(couplets.shape[0]): \n",
    "    itemized.append(tuple(w.lower() for w in re.findall(r\"[\\w']+|[-:.,!?;\\/]\", couplets.loc[i, \"couplet\"])))\n",
    "    \n",
    "couplets[\"itemized\"] = itemized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemizing, without puncuation\n",
    "\n",
    "itemized_sans = []\n",
    "\n",
    "for i in range(couplets.shape[0]):  \n",
    "    itemized_sans.append(tuple(w.lower() for w in re.split(r\"[-:.,\\/\\s]\", couplets.loc[i, \"couplet\"])))\n",
    "\n",
    "couplets[\"itemized_no_punc\"] = itemized_sans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotating the itemized versions with syllable count for each element\n",
    "\n",
    "# with puncutation\n",
    "ann_itemized = [] \n",
    "\n",
    "for i in range(couplets.shape[0]): \n",
    "    ann_element = []\n",
    "    for e in couplets.loc[i, \"itemized\"]: \n",
    "        ann_element.append(tuplize(e.lower()))\n",
    "    ann_itemized.append(tuple(ann_element))\n",
    "\n",
    "couplets[\"annotated\"] = ann_itemized\n",
    "\n",
    "#without puncuation\n",
    "ann_itemized_sans = []\n",
    "\n",
    "for i in range(couplets.shape[0]): \n",
    "    ann_sans_element = []\n",
    "    for e in couplets.loc[i, \"itemized_no_punc\"]: \n",
    "        ann_sans_element.append(tuplize(e.lower()))\n",
    "    ann_itemized_sans.append(tuple(ann_sans_element))\n",
    "    \n",
    "couplets[\"annotated_no_punc\"] = ann_itemized_sans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "couplets.to_csv(\"../data/couplets_formatted.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemizing each line, keeping the puncuation \n",
    "\n",
    "itemized = []\n",
    "\n",
    "for i in range(lines.shape[0]): \n",
    "    itemized.append(tuple(w.lower() for w in re.findall(r\"[\\w']+|[-:.,!?;\\/]\", lines.loc[i, \"line\"])))\n",
    "    \n",
    "lines[\"itemized\"] = itemized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemizing, without puncuation\n",
    "\n",
    "itemized_sans = []\n",
    "\n",
    "for i in range(lines.shape[0]):  \n",
    "    itemized_sans.append(tuple(w.lower() for w in re.split(r\"[-:.,\\/\\s]\", lines.loc[i, \"line\"])))\n",
    "\n",
    "lines[\"itemized_no_punc\"] = itemized_sans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotating the itemized versions with syllable count for each element\n",
    "\n",
    "# with puncutation\n",
    "ann_itemized = [] \n",
    "\n",
    "for i in range(lines.shape[0]): \n",
    "    ann_element = []\n",
    "    for e in lines.loc[i, \"itemized\"]: \n",
    "        ann_element.append(tuplize(e.lower()))\n",
    "    ann_itemized.append(tuple(ann_element))\n",
    "\n",
    "lines[\"annotated\"] = ann_itemized\n",
    "\n",
    "#without puncuation\n",
    "ann_itemized_sans = []\n",
    "\n",
    "for i in range(lines.shape[0]): \n",
    "    ann_sans_element = []\n",
    "    for e in lines.loc[i, \"itemized_no_punc\"]: \n",
    "        ann_sans_element.append(tuplize(e.lower()))\n",
    "    ann_itemized_sans.append(tuple(ann_sans_element))\n",
    "    \n",
    "lines[\"annotated_no_punc\"] = ann_itemized_sans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.to_csv(\"../data/lines_formatted.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating NGram and Individual Word Sequence Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gram = []\n",
    "two_gram = []\n",
    "three_gram = []\n",
    "\n",
    "previous = []\n",
    "next_one = []\n",
    "next_two = []\n",
    "next_three = []\n",
    "\n",
    "one_gram_sans = []\n",
    "two_gram_sans = []\n",
    "three_gram_sans = []\n",
    "\n",
    "previous_sans = []\n",
    "next_one_sans = []\n",
    "next_two_sans = []\n",
    "next_three_sans = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(lines.shape[0]): \n",
    "    \n",
    "    # first with puncuation \n",
    "    for w in range(len(lines.loc[i, \"itemized\"])): \n",
    "        \n",
    "        try: \n",
    "            # add current starting word and ngrams\n",
    "            one_gram.append(lines.loc[i, \"itemized\"][w])\n",
    "\n",
    "            # add previous word\n",
    "            if w == 0: \n",
    "                previous.append(\"\\n\")\n",
    "            else: \n",
    "                previous.append(lines.loc[i, \"itemized\"][w - 1])\n",
    "\n",
    "            # add next word \n",
    "            if w == (len(lines.loc[i, \"itemized\"]) - 1): \n",
    "                next_one.append(\"\\n\")\n",
    "                next_two.append(\"\\n\")\n",
    "                next_three.append(\"\\n\")\n",
    "\n",
    "                two_gram.append(\"\\n\")\n",
    "                three_gram.append(\"\\n\")\n",
    "\n",
    "            elif w == (len(lines.loc[i, \"itemized\"]) - 2): \n",
    "                next_one.append(lines.loc[i, \"itemized\"][w+1])\n",
    "                next_two.append(\"\\n\")\n",
    "                next_three.append(\"\\n\")\n",
    "\n",
    "                two_gram.append(lines.loc[i, \"itemized\"][w : w+2])\n",
    "                three_gram.append(\"\\n\")\n",
    "\n",
    "            elif w == (len(lines.loc[i, \"itemized\"]) - 3): \n",
    "                next_one.append(lines.loc[i, \"itemized\"][w+1])\n",
    "                next_two.append(lines.loc[i, \"itemized\"][w+2])\n",
    "                next_three.append(\"\\n\")\n",
    "\n",
    "                two_gram.append(lines.loc[i, \"itemized\"][w : w+2])\n",
    "                three_gram.append(lines.loc[i, \"itemized\"][w : w+3])\n",
    "\n",
    "            else: \n",
    "                next_one.append(lines.loc[i, \"itemized\"][w+1])\n",
    "                next_two.append(lines.loc[i, \"itemized\"][w+2])\n",
    "                next_three.append(lines.loc[i, \"itemized\"][w+3])\n",
    "\n",
    "                two_gram.append(lines.loc[i, \"itemized\"][w : w+2])\n",
    "                three_gram.append(lines.loc[i, \"itemized\"][w : w+3])\n",
    "    \n",
    "        \n",
    "            # now without puncuation \n",
    "            # add current starting word and ngrams\n",
    "            one_gram_sans.append(lines.loc[i, \"itemized_no_punc\"][w])        \n",
    "\n",
    "            # add previous word\n",
    "            if w == 0: \n",
    "                previous_sans.append(\"\\n\")\n",
    "            else: \n",
    "                previous_sans.append(lines.loc[i, \"itemized_no_punc\"][w - 1])\n",
    "\n",
    "            # add next word \n",
    "            if w == (len(lines.loc[i, \"itemized_no_punc\"]) - 1): \n",
    "                next_one_sans.append(\"\\n\")\n",
    "                next_two_sans.append(\"\\n\")\n",
    "                next_three_sans.append(\"\\n\")\n",
    "\n",
    "                two_gram_sans.append(\"\\n\")\n",
    "                three_gram_sans.append(\"\\n\")\n",
    "\n",
    "            elif w == (len(lines.loc[i, \"itemized_no_punc\"]) - 2): \n",
    "                next_one_sans.append(lines.loc[i, \"itemized_no_punc\"][w+1])\n",
    "                next_two_sans.append(\"\\n\")\n",
    "                next_three_sans.append(\"\\n\")\n",
    "\n",
    "                two_gram_sans.append(lines.loc[i, \"itemized_no_punc\"][w : w+2])\n",
    "                three_gram_sans.append(\"\\n\")\n",
    "\n",
    "            elif w == (len(lines.loc[i, \"itemized_no_punc\"]) - 3): \n",
    "                next_one_sans.append(lines.loc[i, \"itemized_no_punc\"][w+1])\n",
    "                next_two_sans.append(lines.loc[i, \"itemized_no_punc\"][w+2])\n",
    "                next_three_sans.append(\"\\n\")\n",
    "\n",
    "                two_gram_sans.append(lines.loc[i, \"itemized_no_punc\"][w : w+2])\n",
    "                three_gram_sans.append(lines.loc[i, \"itemized_no_punc\"][w : w+3])\n",
    "\n",
    "            else: \n",
    "                next_one_sans.append(lines.loc[i, \"itemized_no_punc\"][w+1])\n",
    "                next_two_sans.append(lines.loc[i, \"itemized_no_punc\"][w+2])\n",
    "                next_three_sans.append(lines.loc[i, \"itemized_no_punc\"][w+3])\n",
    "\n",
    "                two_gram_sans.append(lines.loc[i, \"itemized_no_punc\"][w : w+2])\n",
    "                three_gram_sans.append(lines.loc[i, \"itemized_no_punc\"][w : w+3])\n",
    "        \n",
    "        except: \n",
    "            pass \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sequence = {\"previous_word\": previous, \"one_word\": one_gram, \"one_word_next\": next_one, \n",
    "                 \"two_word\": two_gram, \"two_word_next\": next_two, \"three_word\": three_gram, \n",
    "                 \"three_word_next\": next_three}\n",
    "\n",
    "word_sequence_np = {\"np_previous_word\": previous_sans, \n",
    "                 \"np_one_word\": one_gram_sans, \"np_one_word_next\": next_one_sans, \n",
    "                 \"np_two_word\": two_gram_sans, \"np_two_word_next\": next_two_sans, \n",
    "                 \"np_three_word\": three_gram_sans, \"np_three_word_next\": next_three_sans}\n",
    "\n",
    "ann_word_seq = {\"previous_word\": list(map(tuplize, previous)), \"one_word\": list(map(tuplize, one_gram)), \n",
    "                \"one_word_next\": list(map(tuplize, next_one)), \"two_word\": list(map(tuplize, two_gram)), \n",
    "                \"two_word_next\": list(map(tuplize, next_two)), \"three_gram\": list(map(tuplize, three_gram)), \n",
    "                \"three_word_next\": list(map(tuplize, next_three))}\n",
    "\n",
    "ann_word_seq_np = {\"np_previous_word\": list(map(tuplize, previous_sans)), \"np_one_word\": list(map(tuplize, one_gram_sans)), \n",
    "                   \"np_one_word_next\": list(map(tuplize, next_one_sans)), \"np_two_word\": list(map(tuplize, two_gram_sans)), \n",
    "                   \"np_two_word_next\": list(map(tuplize, next_two_sans)), \"np_three_word\": list(map(tuplize, three_gram_sans)), \n",
    "                   \"np_three_word_next\": list(map(tuplize, next_three_sans))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sequence_df = pd.DataFrame(word_sequence)\n",
    "\n",
    "word_sequence_np_df = pd.DataFrame(word_sequence_np)\n",
    "\n",
    "ann_word_seq_df = pd.DataFrame(ann_word_seq)\n",
    "\n",
    "ann_word_seq_np_df = pd.DataFrame(ann_word_seq_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sequence_df.to_csv(\"../data/word_sequences.csv\", index = False)\n",
    "\n",
    "word_sequence_np_df.to_csv(\"../data/word_sequences_np.csv\", index = False)\n",
    "\n",
    "ann_word_seq_df.to_csv(\"../data/word_sequences_ann.csv\", index = False)\n",
    "\n",
    "ann_word_seq_np_df.to_csv(\"../data/word_sequences_ann_np.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
